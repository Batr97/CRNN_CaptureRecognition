{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnibsFibU2Bl"
      },
      "source": [
        "# Подготовка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SHH_hdPoN1Uc"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "import glob\n",
        "from tqdm import tqdm\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import os\n",
        "import cv2 as cv \n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKUOTiHoKy-U",
        "outputId": "e1e0e457-ab72-4de4-f6c2-15f69c96458f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s3EsTTNDM0-n"
      },
      "outputs": [],
      "source": [
        "# !unzip /content/gdrive/MyDrive/MADE/DL_intensive/laba-dataset.zip -d /content/gdrive/MyDrive/MADE/DL_intensive/laba_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CuHLXPoGc9Pr"
      },
      "outputs": [],
      "source": [
        "def normalize(arr):\n",
        "    \"\"\"\n",
        "    Linear normalization\n",
        "    http://en.wikipedia.org/wiki/Normalization_%28image_processing%29\n",
        "    \"\"\"\n",
        "    arr = arr.astype('float')\n",
        "    # Do not touch the alpha channel\n",
        "    for i in range(3):\n",
        "        minval = arr[...,i].min()\n",
        "        maxval = arr[...,i].max()\n",
        "        if minval != maxval:\n",
        "            arr[...,i] -= minval\n",
        "            arr[...,i] *= (255.0/(maxval-minval))\n",
        "    return arr\n",
        "\n",
        "def batch_mean_and_sd(loader):\n",
        "    \n",
        "    cnt = 0\n",
        "    fst_moment = torch.empty(3)\n",
        "    snd_moment = torch.empty(3)\n",
        "\n",
        "    for images, _ in loader:\n",
        "        b, c, h, w = images.shape\n",
        "        nb_pixels = b * h * w\n",
        "        sum_ = torch.sum(images, dim=[0, 2, 3])\n",
        "        sum_of_square = torch.sum(images ** 2,\n",
        "                                  dim=[0, 2, 3])\n",
        "        fst_moment = (cnt * fst_moment + sum_) / (\n",
        "                      cnt + nb_pixels)\n",
        "        snd_moment = (cnt * snd_moment + sum_of_square) / (\n",
        "                            cnt + nb_pixels)\n",
        "        cnt += nb_pixels\n",
        "\n",
        "    mean, std = fst_moment, torch.sqrt(\n",
        "      snd_moment - fst_moment ** 2)        \n",
        "    return mean,std"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "sm2G9oKljna0"
      },
      "outputs": [],
      "source": [
        "transform_img = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # here do not use transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4NH8lRhzOrQk"
      },
      "outputs": [],
      "source": [
        "path = '/content/gdrive/MyDrive/MADE/DL_intensive/laba_dataset/samples'\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, path, targets, transform = None):\n",
        "        self.imgs_path = path\n",
        "        self.file_list = path\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path = self.file_list[idx]\n",
        "        # img = cv.imread(path) \n",
        "        img = Image.open(path)\n",
        "        img = img.convert('RGB')\n",
        "        img = np.array(img) \n",
        "        label = self.targets[idx]\n",
        "        # mean = img.mean()\n",
        "        # std = img.std()\n",
        "        # img = (img - mean) / std\n",
        "        # img = torch.from_numpy(img)\n",
        "        # img=img.float()\n",
        "        if self.transform:\n",
        "          img = self.transform(img)\n",
        "        \n",
        "        # return {\n",
        "        #     \"images\": torch.tensor(img, dtype=torch.float),\n",
        "        #     \"targets\": torch.tensor(label, dtype=torch.long),\n",
        "        # }\n",
        "        return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# dataset = CustomDataset(path, transform_img)\n",
        "# dataloader = DataLoader(dataset, batch_size=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "Cn9QMVfHzv5U",
        "outputId": "7a05173a-769a-4ab5-a72b-6a792d18ad93"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e3028835e3ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# lets calculate mean and std for the dataaset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_mean_and_sd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean and std: \\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
          ]
        }
      ],
      "source": [
        "# lets calculate mean and std for the dataaset\n",
        "mean, std = batch_mean_and_sd(dataloader)\n",
        "print(\"mean and std: \\n\", mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0iWYPyLk1hM5"
      },
      "outputs": [],
      "source": [
        "transform_img_normal = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean = [0.7152, 0.7152, 0.7152],\n",
        "                         std= [0.3137, 0.3137, 0.3137])\n",
        "])\n",
        "\n",
        "# dataset = CustomDataset(path, transform_img_normal)\n",
        "# dataloader = DataLoader(dataset, batch_size=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "n8DF_OZr3eQ7"
      },
      "outputs": [],
      "source": [
        "# train_size = int(0.75 * len(dataset))\n",
        "# test_size = len(dataset) - train_size\n",
        "\n",
        "image_files = glob.glob(os.path.join(path, \"*.png\"))\n",
        "targets_orig = [x.split('/')[-1].split('.')[0] for x in image_files]\n",
        "targets = [[c for c in x] for x in targets_orig]\n",
        "targets_flat = [c for clist in targets for c in clist]\n",
        "\n",
        "lbl_enc = preprocessing.LabelEncoder()\n",
        "lbl_enc.fit(targets_flat)\n",
        "targets_enc = [lbl_enc.transform(x) for x in targets]\n",
        "targets_enc = np.array(targets_enc)\n",
        "targets_enc = targets_enc + 1\n",
        "\n",
        "train_imgs, test_imgs, train_targets, test_targets, _, test_targets_orig = train_test_split(image_files, targets_enc, targets_orig, test_size=0.1, random_state=42)\n",
        "\n",
        "train_dataset = CustomDataset(train_imgs, train_targets, transform_img_normal)\n",
        "\n",
        "train_loader = dataloader = DataLoader(train_dataset, batch_size=4, pin_memory=True, shuffle=True)\n",
        "# torch.utils.data.DataLoader(\n",
        "#     train_dataset,\n",
        "#     batch_size=config.BATCH_SIZE,\n",
        "#     num_workers=config.NUM_WORKERS,\n",
        "#     shuffle=True,\n",
        "# )\n",
        "test_dataset = CustomDataset(test_imgs, test_targets, transform_img_normal)\n",
        "\n",
        "# dataset.ClassificationDataset(\n",
        "#     image_paths=test_imgs,\n",
        "#     targets=test_targets,\n",
        "#     resize=(config.IMAGE_HEIGHT, config.IMAGE_WIDTH),\n",
        "# )\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, pin_memory=True, shuffle=False)\n",
        "# torch.utils.data.DataLoader(\n",
        "#     test_dataset,\n",
        "#     batch_size=config.BATCH_SIZE,\n",
        "#     num_workers=config.NUM_WORKERS,\n",
        "#     shuffle=False,\n",
        "# )\n",
        "\n",
        "\n",
        "# train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "# train_loader = DataLoader(train_dataset, batch_size=4, pin_memory=True)\n",
        "# test_loader = DataLoader(test_dataset, batch_size=4, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "O5-JXCADOrjk",
        "outputId": "83f9b7d3-1a53-4d37-8c6d-c644646655b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "images torch.Size([4, 3, 50, 200])\n",
            "Label: tensor([[ 3,  2, 13, 11, 19],\n",
            "        [19,  4, 10, 16, 16],\n",
            "        [ 2,  7, 15,  4,  6],\n",
            "        [ 4,  5, 15,  9, 18]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1055473040>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display image and label.\n",
        "data = next(iter(train_loader))\n",
        "# img = data['images']\n",
        "# label = data['targets']\n",
        "img = data[0]\n",
        "label = data[1]\n",
        "\n",
        "print('images', img.shape)\n",
        "print(f\"Label: {label}\")\n",
        "plt.imshow(img[0].permute(1,2,0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXOp39Gw4aKn"
      },
      "source": [
        "# Создание и обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shTBThkR7z5v"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "predictions after model are the same as for indus, so make the function decoder the same as he\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OF9kJOM6g0EA"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "alphabet = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
        "alph = lbl_enc.classes_\n",
        "n_classes = len(alph) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tab1_rNto2u2"
      },
      "outputs": [],
      "source": [
        "def train_fn(model, data_loader, optimizer):\n",
        "    model.train()\n",
        "    fin_loss = 0\n",
        "    tk0 = tqdm(data_loader, total=len(data_loader))\n",
        "    for data in tk0:\n",
        "        data[0] = data[0].to(device)\n",
        "        data[1] = data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out, loss = model(data[0], data[1])\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)\n",
        "        optimizer.step()\n",
        "        fin_loss += loss.item()\n",
        "    return fin_loss / len(data_loader)\n",
        "\n",
        "\n",
        "def eval_fn(model, data_loader):\n",
        "    model.eval()\n",
        "    fin_loss = 0\n",
        "    fin_preds = []\n",
        "    with torch.no_grad():\n",
        "      tk0 = tqdm(data_loader, total=len(data_loader))\n",
        "      for data in tk0:\n",
        "          data[0] = data[0].to(device)\n",
        "          data[1] = data[1].to(device)\n",
        "          batch_preds, loss = model(data[0], data[1])\n",
        "          fin_loss += loss.item()\n",
        "          fin_preds.append(batch_preds)\n",
        "      return fin_preds, fin_loss / len(data_loader)\n",
        "\n",
        "def train():\n",
        "  model = CRNN()\n",
        "  model.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "      optimizer, factor=0.8, patience=5, verbose=True\n",
        "  )\n",
        "  for epoch in range(2):\n",
        "      train_loss = train_fn(model, train_loader, optimizer)\n",
        "      valid_preds, test_loss = eval_fn(model, test_loader)\n",
        "      valid_captcha_preds = []\n",
        "      for vp in valid_preds:\n",
        "          current_preds = decode_predictions(vp, lbl_enc)\n",
        "          valid_captcha_preds.extend(current_preds)\n",
        "      combined = list(zip(test_targets_orig, valid_captcha_preds))\n",
        "      print(combined[:10])\n",
        "      test_dup_rem = [remove_duplicates(c) for c in test_targets_orig]\n",
        "      accuracy = metrics.accuracy_score(test_dup_rem, valid_captcha_preds)\n",
        "      print(\n",
        "          f\"Epoch={epoch}, Train Loss={train_loss}, Test Loss={test_loss} Accuracy={accuracy}\"\n",
        "      )\n",
        "      scheduler.step(test_loss)\n",
        "\n",
        "def remove_duplicates(x):\n",
        "    if len(x) < 2:\n",
        "        return x\n",
        "    fin = \"\"\n",
        "    for j in x:\n",
        "        if fin == \"\":\n",
        "            fin = j\n",
        "        else:\n",
        "            if j == fin[-1]:\n",
        "                continue\n",
        "            else:\n",
        "                fin = fin + j\n",
        "    return fin\n",
        "\n",
        "def decode_predictions(preds, encoder):\n",
        "    preds = preds.permute(1, 0, 2)\n",
        "    # print('before softmax and after permuattion', preds.shape)\n",
        "    preds = torch.softmax(preds, 2)\n",
        "    # print('why softmax?', preds.shape)\n",
        "    # print('preds', preds[0])\n",
        "    preds = torch.argmax(preds, 2)\n",
        "    # print('preds.argmax', preds[0])\n",
        "    preds = preds.detach().cpu().numpy()\n",
        "    cap_preds = []\n",
        "    for j in range(preds.shape[0]):\n",
        "        temp = []\n",
        "        for k in preds[j, :]:\n",
        "            k = k - 1\n",
        "            if k == -1:\n",
        "                temp.append(\"§\")\n",
        "            else:\n",
        "                p = encoder.inverse_transform([k])[0]\n",
        "                temp.append(p)\n",
        "        tp = \"\".join(temp).replace(\"§\", \"\")\n",
        "        cap_preds.append(remove_duplicates(tp))\n",
        "    # print('temp', temp)\n",
        "    # print('tp', tp)\n",
        "\n",
        "    return cap_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uIl8Y1x6Orx6"
      },
      "outputs": [],
      "source": [
        "class BidirectionalLSTM(nn.Module):\n",
        "\n",
        "    def __init__(self, nIn, nHidden, nOut):\n",
        "        super(BidirectionalLSTM, self).__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(nIn, nHidden, bidirectional=True)\n",
        "        self.embedding = nn.Linear(nHidden * 2, nOut)\n",
        "\n",
        "    def forward(self, input):\n",
        "        recurrent, _ = self.rnn(input)\n",
        "        b, T, h = recurrent.size()\n",
        "        t_rec = recurrent.view(T * b, h)\n",
        "        # t_rec = recurrent.permute(1, 0 ,2)\n",
        "\n",
        "        output = self.embedding(t_rec)  # [T * b, nOut]\n",
        "        output = output.view(T, b, -1)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class CRNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(CRNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=(3, 3), padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(3, 3), padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=(3, 3), padding=1)\n",
        "        self.pool4 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Conv2d(256, 256, kernel_size=(2, 2), padding=0)\n",
        "        self.pool5 = nn.MaxPool2d(2, 1)\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "        self.conv6 = nn.Conv2d(256, 256, kernel_size=(4, 2))\n",
        "        self.rnn = nn.LSTM(256, 128, num_layers=2, bidirectional=True, batch_first=True)\n",
        "        self.linear = nn.Linear(256, n_classes)\n",
        "\n",
        "    def forward(self, x, targets):\n",
        "        batch_size = x.shape[0]\n",
        "        out = self.conv1(x)\n",
        "        out = self.pool1(out)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.pool2(out)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.pool4(out)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.conv5(out)\n",
        "        out = self.pool5(out)\n",
        "        out = self.bn5(out)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = self.conv6(out)\n",
        "        out = F.leaky_relu(out)\n",
        "        out = out.squeeze(2)\n",
        "        out = out.permute(0, 2, 1) # [b, w, h, c]\n",
        "        out, _ = self.rnn(out)\n",
        "        out = self.linear(out)\n",
        "        out = out.permute(1, 0, 2)\n",
        "        log_probs = F.log_softmax(out, 2)\n",
        "        input_lengths = torch.full(size=(4,), fill_value=log_probs.size(0), dtype=torch.int32)\n",
        "        target_lengths = torch.full(size=(4,), fill_value=targets.size(1), dtype=torch.int32)\n",
        "        loss = nn.CTCLoss(blank=10)(log_probs, targets, input_lengths, target_lengths)\n",
        "\n",
        "        return out, loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gI3N5IWFDaDA",
        "outputId": "5d90f950-cdd5-4470-a4cd-c39dfc247a88"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.1-py3-none-any.whl (517 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.13.1+cu116)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (23.0)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.1\n"
          ]
        }
      ],
      "source": [
        "pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rTRff2qFqUEu"
      },
      "outputs": [],
      "source": [
        "from torchmetrics.functional import char_error_rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBH-hgQYkAAk",
        "outputId": "ffa970ae-6698-4aa3-a477-9e8616dcac85"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:21<00:00,  2.87it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 10.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0, train loss: 3.1293254632216234, test loss: 2.945602875489455\n",
            "[('dnxdp', 'd'), ('m3wfw', 'd'), ('7bwm2', 'd'), ('3nnpw', 'd'), ('pm363', 'd'), ('x362g', 'd'), ('g247w', 'd'), ('bgb48', 'd'), ('8cm46', 'd'), ('4n3mn', 'd')]\n",
            "Epoch=0, Train Loss=3.1293254632216234, Test Loss=2.945602875489455 Accuracy=0.0 CER=4.625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:01<00:00,  3.83it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.49it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, train loss: 2.9189328245627575, test loss: 2.9338804941910963\n",
            "[('dnxdp', 'd'), ('m3wfw', 'd'), ('7bwm2', 'd'), ('3nnpw', 'd'), ('pm363', 'd'), ('x362g', 'd'), ('g247w', 'd'), ('bgb48', 'd'), ('8cm46', 'd'), ('4n3mn', 'd')]\n",
            "Epoch=1, Train Loss=2.9189328245627575, Test Loss=2.9338804941910963 Accuracy=0.0 CER=4.625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:00<00:00,  3.87it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00,  9.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2, train loss: 2.8480534976364202, test loss: 2.8224500050911536\n",
            "[('dnxdp', 'd'), ('m3wfw', 'd'), ('7bwm2', 'd'), ('3nnpw', 'd'), ('pm363', 'd'), ('x362g', 'd'), ('g247w', 'd'), ('bgb48', 'd'), ('8cm46', 'd'), ('4n3mn', 'd')]\n",
            "Epoch=2, Train Loss=2.8480534976364202, Test Loss=2.8224500050911536 Accuracy=0.0 CER=4.625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [00:59<00:00,  3.94it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3, train loss: 2.3985045154889426, test loss: 1.959514292386862\n",
            "[('dnxdp', 'd'), ('m3wfw', 'd'), ('7bwm2', 'd'), ('3nnpw', 'd'), ('pm363', 'd'), ('x362g', 'd'), ('g247w', 'd'), ('bgb48', 'd'), ('8cm46', 'd'), ('4n3mn', 'd')]\n",
            "Epoch=3, Train Loss=2.3985045154889426, Test Loss=1.959514292386862 Accuracy=0.0 CER=4.625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [00:59<00:00,  3.94it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4, train loss: 1.106950832833337, test loss: 0.3418266664330776\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dnd3dwdfdwd'), ('7bwm2', 'd7dbdwdnd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdnd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdnd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=4, Train Loss=1.106950832833337, Test Loss=0.3418266664330776 Accuracy=0.0 CER=0.5694572329521179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:02<00:00,  3.74it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 5, train loss: 0.06320766338871585, test loss: 0.003984422637866094\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdnd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=5, Train Loss=0.06320766338871585, Test Loss=0.003984422637866094 Accuracy=0.0 CER=0.5510948896408081\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [00:59<00:00,  3.95it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 6, train loss: -0.05348807371133923, test loss: -0.059146215279514976\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdnd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=6, Train Loss=-0.05348807371133923, Test Loss=-0.059146215279514976 Accuracy=0.0 CER=0.5437158346176147\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:01<00:00,  3.81it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 10.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 7, train loss: -0.10551659335804164, test loss: -0.10287077796573822\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=7, Train Loss=-0.10551659335804164, Test Loss=-0.10287077796573822 Accuracy=0.0 CER=0.5419707894325256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:01<00:00,  3.84it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 11.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 8, train loss: -0.12124207560928205, test loss: -0.10970070743216918\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=8, Train Loss=-0.12124207560928205, Test Loss=-0.10970070743216918 Accuracy=0.0 CER=0.5419707894325256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:16<00:00,  3.06it/s]\n",
            "100%|██████████| 26/26 [00:03<00:00,  8.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 9, train loss: -0.12276643441201976, test loss: -0.11128565235636555\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=9, Train Loss=-0.12276643441201976, Test Loss=-0.11128565235636555 Accuracy=0.0 CER=0.5419707894325256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:12<00:00,  3.21it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 10, train loss: -0.1130049961587239, test loss: 0.05054385573245012\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dwdndpdwd'), ('pm363', 'dpdmd6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdwd4d6d'), ('4n3mn', 'd4dnd3dwdnd')]\n",
            "Epoch=10, Train Loss=-0.1130049961587239, Test Loss=0.05054385573245012 Accuracy=0.0 CER=0.5558608174324036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:00<00:00,  3.84it/s]\n",
            "100%|██████████| 26/26 [00:03<00:00,  8.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 11, train loss: -0.12205477902848814, test loss: -0.11748539953707503\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=11, Train Loss=-0.12205477902848814, Test Loss=-0.11748539953707503 Accuracy=0.0 CER=0.5419707894325256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:00<00:00,  3.84it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 12, train loss: -0.1326014330574813, test loss: -0.10409213914177738\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=12, Train Loss=-0.1326014330574813, Test Loss=-0.10409213914177738 Accuracy=0.0 CER=0.543795645236969\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:00<00:00,  3.87it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 11.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 13, train loss: -0.13388551930343717, test loss: -0.12040298159878987\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=13, Train Loss=-0.13388551930343717, Test Loss=-0.12040298159878987 Accuracy=0.0 CER=0.5419707894325256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:00<00:00,  3.88it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 10.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 14, train loss: -0.12034185464955612, test loss: -0.11576414971540754\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=14, Train Loss=-0.12034185464955612, Test Loss=-0.11576414971540754 Accuracy=0.0 CER=0.5420475602149963\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:00<00:00,  3.84it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 15, train loss: -0.13066765504519048, test loss: -0.12052406774511418\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=15, Train Loss=-0.13066765504519048, Test Loss=-0.12052406774511418 Accuracy=0.0 CER=0.5419707894325256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [01:01<00:00,  3.82it/s]\n",
            "100%|██████████| 26/26 [00:03<00:00,  8.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 16, train loss: -0.13323813181521738, test loss: -0.11491303520205502\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=16, Train Loss=-0.13323813181521738, Test Loss=-0.11491303520205502 Accuracy=0.0 CER=0.5428832173347473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [00:59<00:00,  3.91it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 17, train loss: -0.12370349905795903, test loss: -0.11093054027654804\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=17, Train Loss=-0.12370349905795903, Test Loss=-0.11093054027654804 Accuracy=0.0 CER=0.5420475602149963\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [00:59<00:00,  3.95it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 12.30it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 18, train loss: -0.13083314889858866, test loss: -0.12059138922128253\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=18, Train Loss=-0.13083314889858866, Test Loss=-0.12059138922128253 Accuracy=0.0 CER=0.5419707894325256\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/234 [00:00<?, ?it/s]<ipython-input-10-6e3cd6adde2d>:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(img, dtype=torch.float), torch.tensor(label, dtype=torch.long)\n",
            "100%|██████████| 234/234 [00:59<00:00,  3.94it/s]\n",
            "100%|██████████| 26/26 [00:02<00:00, 10.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 19, train loss: -0.1353358458667898, test loss: -0.12640478641081315\n",
            "[('dnxdp', 'dndxdpd'), ('m3wfw', 'dmd3dwdfdwd'), ('7bwm2', 'd7dbdwdmd2d'), ('3nnpw', 'd3dndndpdwd'), ('pm363', 'dpdmd3d6d3d'), ('x362g', 'dxd3d6d2dgd'), ('g247w', 'dgd2d4d7dwd'), ('bgb48', 'dbdgdbd4d8d'), ('8cm46', 'd8dcdmd4d6d'), ('4n3mn', 'd4dnd3dmdnd')]\n",
            "Epoch=19, Train Loss=-0.1353358458667898, Test Loss=-0.12640478641081315 Accuracy=0.0 CER=0.5419707894325256\n",
            "final cer tensor(0.5420)\n"
          ]
        }
      ],
      "source": [
        "model = CRNN()\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, factor=0.8, patience=5, verbose=True\n",
        ")\n",
        "for epoch in range(20):\n",
        "    train_loss = train_fn(model, train_loader, optimizer)\n",
        "    valid_preds, test_loss = eval_fn(model, test_loader)\n",
        "    print(f'epoch {epoch}, train loss: {train_loss}, test loss: {test_loss}')\n",
        "    valid_captcha_preds = []\n",
        "    # print('len valid_preds', len(valid_preds))\n",
        "    # print('valid_preds', valid_preds[0].shape)\n",
        "    for vp in valid_preds:\n",
        "        current_preds = decode_predictions(vp, lbl_enc)\n",
        "        valid_captcha_preds.extend(current_preds)\n",
        "    # print('current_preds', current_preds)\n",
        "    # print('valid_captcha_preds', valid_captcha_preds)\n",
        "    combined = list(zip(test_targets_orig, valid_captcha_preds))\n",
        "    print(combined[:10])\n",
        "    test_dup_rem = [remove_duplicates(c) for c in test_targets_orig]\n",
        "    # print('test_dup_rem', test_dup_rem)\n",
        "    accuracy = accuracy_score(test_dup_rem, valid_captcha_preds)\n",
        "    cer = char_error_rate(test_dup_rem, valid_captcha_preds)\n",
        "    print(\n",
        "        f\"Epoch={epoch}, Train Loss={train_loss}, Test Loss={test_loss} Accuracy={accuracy} CER={cer}\"\n",
        "    )\n",
        "    scheduler.step(test_loss)\n",
        "print('final cer', cer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ow3_ggtjr9A"
      },
      "source": [
        "Улучшение модели было произведено засчет клиппинга градиентов и увеличением числа эпох, до этого прогнозировались пустые значения или одна и та же последовательность длиной 1. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lf8vOFsLBEm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2js_dsyLBJv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9ce8a0452749bc4b5f03b6e5df3595212fc7910e9903d106d143563d7de8650a"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
